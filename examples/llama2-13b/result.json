{
  "best_batch_p": 2,
  "best_batch_d": 356,
  "best_partition": [
    16,
    13,
    11
  ],
  "best_delta": 0.012967713022485172,
  "best_cost": 0.636539049838353,
  "max_stage": 3,
  "max_layer_per_gpu": 17,
  "prefill_latency": 0.7864780173885602,
  "decode_latency": 0.6714326716844181,
  "total_prefill_latency": 0.9124134434335305,
  "total_decode_latency": 0.7973680977293884,
  "dfs_count": 0,
  "dfs_original_count": 307,
  "solve_time": 0.09913134574890137,
  "meets_slo": true,
  "config_params": {
    "model": "Llama-2-13b",
    "gpu_name": "t4-pcie-16gb",
    "dtype": "w16a16e16",
    "max_chunk_size": 256,
    "slo_prefill": 2.0,
    "slo_decode": 0.8,
    "tp_size": 1,
    "pp_size": 1,
    "sp_size": 1,
    "dp_size": 1,
    "algorithm": "bucket",
    "verbose": false,
    "quiet": false
  }
}