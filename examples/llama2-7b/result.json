{
  "best_batch_p": 6,
  "best_batch_d": 423,
  "best_partition": [
    24,
    8
  ],
  "best_delta": 0.018625778447652896,
  "best_cost": 0.33481499789309044,
  "max_stage": 2,
  "max_layer_per_gpu": 23,
  "prefill_latency": 1.21958687414785,
  "decode_latency": 0.4313633293128205,
  "total_prefill_latency": 1.2882126525955029,
  "total_decode_latency": 0.4999891077604734,
  "dfs_count": 0,
  "dfs_original_count": 25,
  "solve_time": 2.0871331691741943,
  "meets_slo": true,
  "config_params": {
    "model": "Llama-2-7b",
    "gpu_name": "t4-pcie-16gb",
    "dtype": "w16a16e16",
    "max_chunk_size": 256,
    "slo_prefill": 1.5,
    "slo_decode": 0.5,
    "tp_size": 1,
    "pp_size": 1,
    "sp_size": 1,
    "dp_size": 1,
    "algorithm": "both",
    "verbose": false,
    "quiet": false
  }
}